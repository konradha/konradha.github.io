<!doctype html><html class="not-ready lg:text-base" style=--bg:#f8f5d7 lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Solving PDEs using ✨Machine Learning✨ - א</title><meta name=theme-color><meta name=description content="At university some of us choose to learn how to solve PDEs numerically.
Usually, you pass years learning about approximation
theory, test functions, finite elements, C++, functional analysis, visualization libraries, parallelization. Finally,
you put it together to solve heat or wave equations. Then, when you&rsquo;re more of a practitioner in industry, you fall back to using Ansys.
In 2023 however, we can just take our good ol&rsquo; reliable Pytorch and have Adam walk the space that minimizes the
residuals we define for a given PDE."><meta name=author content="א"><link rel="preload stylesheet" as=style href=https://konradha.com/main.min.css><link rel=preload as=image href=https://konradha.com/theme.png><script defer src=https://konradha.com/highlight.min.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://konradha.com/favicon.ico><link rel=apple-touch-icon href=https://konradha.com/apple-touch-icon.png><meta name=generator content="Hugo 0.147.6"><meta itemprop=name content="Solving PDEs using ✨Machine Learning✨"><meta itemprop=description content="At university some of us choose to learn how to solve PDEs numerically. Usually, you pass years learning about approximation theory, test functions, finite elements, C++, functional analysis, visualization libraries, parallelization. Finally, you put it together to solve heat or wave equations. Then, when you’re more of a practitioner in industry, you fall back to using Ansys. In 2023 however, we can just take our good ol’ reliable Pytorch and have Adam walk the space that minimizes the residuals we define for a given PDE."><meta itemprop=datePublished content="2023-06-07T18:57:35+02:00"><meta itemprop=dateModified content="2023-06-07T18:57:35+02:00"><meta itemprop=wordCount content="1759"><meta property="og:url" content="https://konradha.com/posts/pinns/"><meta property="og:site_name" content="א"><meta property="og:title" content="Solving PDEs using ✨Machine Learning✨"><meta property="og:description" content="At university some of us choose to learn how to solve PDEs numerically. Usually, you pass years learning about approximation theory, test functions, finite elements, C++, functional analysis, visualization libraries, parallelization. Finally, you put it together to solve heat or wave equations. Then, when you’re more of a practitioner in industry, you fall back to using Ansys. In 2023 however, we can just take our good ol’ reliable Pytorch and have Adam walk the space that minimizes the residuals we define for a given PDE."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-07T18:57:35+02:00"><meta property="article:modified_time" content="2023-06-07T18:57:35+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Solving PDEs using ✨Machine Learning✨"><meta name=twitter:description content="At university some of us choose to learn how to solve PDEs numerically. Usually, you pass years learning about approximation theory, test functions, finite elements, C++, functional analysis, visualization libraries, parallelization. Finally, you put it together to solve heat or wave equations. Then, when you’re more of a practitioner in industry, you fall back to using Ansys. In 2023 however, we can just take our good ol’ reliable Pytorch and have Adam walk the space that minimizes the residuals we define for a given PDE."><link rel=canonical href=https://konradha.com/posts/pinns/></head><link rel=stylesheet href=/css/fix.css><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center"><div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center"><a class="-translate-y-[1px] text-2xl font-medium" href=https://konradha.com/>א</a><div class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#f8f5d7".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"><a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"><article><header class=mb-14><h1 class="!my-0 pb-2.5">Solving PDEs using ✨Machine Learning✨</h1><div class="text-xs antialiased opacity-60"><time>Jun 7, 2023</time></div></header><section><p>At university some of us choose to learn how to solve PDEs numerically.
Usually, you pass years learning about approximation
theory, test functions, finite elements, C++, functional analysis, visualization libraries, parallelization. Finally,
you put it together to solve heat or wave equations. Then, when you&rsquo;re more of a practitioner in industry, you fall back to using Ansys.
In 2023 however, we can just take our good ol&rsquo; reliable Pytorch and <em>have Adam walk the space that minimizes the
residuals we define for a given PDE</em>.</p><p>And: you don&rsquo;t even need any external data. Choosing your points in a
<a href=https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html>smart manner</a>
(ie. not sampling from some centered distribution), you can even deduce
very nice bounds on convergence and precision of the solution you arrive at. Tremendous.</p><p>Let&rsquo;s not talk about the hard numerical analysis terms and other concerns. I want to give an overview
on how you can <em>quickly</em> build solvers for instances of PDE problems on your own.</p><h3 id=the-setup>The setup</h3><p>We start from a very common formulation of a PDE problem: Take some operator L defined on some
region O, have it equal some source term f. Define some boundary conditions that are close to your application and you&rsquo;re
done.</p><p>Our goal is to build a pattern that lets us include everything needed to instantiate the problem, understand
the geometry and fix parameters in a way that makes the entire computation reproduceable.
We also want to reuse as much as possible from our existing knowledge of how our ML framework works.</p><p>In the beginning, you don&rsquo;t need to worry too much about the model architecture you&rsquo;ll need to use. Just take
a vanilla MLP with several hidden layers. Choosing your activation functions carefully however, you might even have some <a href=https://arxiv.org/abs/2104.08938>guarantees</a>
on how well this works on paper.</p><p>I&rsquo;m going to show the main ideas in somewhat Pythonic pseudocode. For details, follow <a href=https://github.com/konradha/DLSC/blob/main/Pinns.ipynb>this</a>
tutorial to build a solver for the 1D heat equation. This <a href=https://github.com/mroberto166/CAMLab-DLSCTutorials>collection</a> of tutorials
is probably a little more polished and complete. If you actually want to see the code running, just clone
one of the repos I&rsquo;m linking and see how the models converge to a specific analytic solution.</p><h3 id=implementation>Implementation</h3><p>Now: Roll your own PINN object. &ldquo;PINN&rdquo; is short for <a href=https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/>Phyics-informed neural network</a>.
This approach may be extended to your given problem fairly quickly
(ie. another PDE or maybe an inverse problem by introducing another neural net that approximates your
parameter &mldr; see the lecture I&rsquo;ve linked below for more possibilities).</p><p>Create an object that contains your solver, the parameter space and all the methods
you&rsquo;d need to compute your problem: On instantiation you want to define the most important variables; ie.
how many boundary points you want to sample on, inner points, the depth of your network.
<code>assemble_datasets</code> defines a method to fill our object with data representing our problem&rsquo;s region.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PINN</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, n_int, n_sb, n_tb, n_out):
</span></span><span style=display:flex><span>        <span style=color:#75715e># domain extrema here: using a rectangular 2d geometry</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>domain_extrema <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>0</span>, T],  <span style=color:#75715e># Time dimension</span>
</span></span><span style=display:flex><span>                                            [<span style=color:#ae81ff>0</span>, X]])  <span style=color:#75715e># Space dimension</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>approximate_solution <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>NN 
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>training_set_sb, self<span style=color:#f92672>.</span>training_set_tb, self<span style=color:#f92672>.</span>training_set_int <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>assemble_datasets()
</span></span></code></pre></div><p>We want to make use of the existing ML patterns to train models. Hence, we adapt our class structure
such that we can call a single <code>compute_loss</code> function that does all of the numerical compute.
<code>tb</code> and <code>sb</code> are short for temporal and spatial boundary respectively.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, num_epochs, optimizer, verbose<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>        history <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> j, ((inp_train_sb, u_train_sb), (inp_train_tb, u_train_tb),
</span></span><span style=display:flex><span>                    (inp_train_int, u_train_int)) <span style=color:#f92672>in</span> enumerate(
</span></span><span style=display:flex><span>                            zip(self<span style=color:#f92672>.</span>training_set_sb, self<span style=color:#f92672>.</span>training_set_tb, self<span style=color:#f92672>.</span>training_set_int)):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>closure</span>():
</span></span><span style=display:flex><span>                    optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>                    loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(inp_train_sb, u_train_sb,
</span></span><span style=display:flex><span>                                             inp_train_tb, u_train_tb, inp_train_int, verbose<span style=color:#f92672>=</span>verbose)
</span></span><span style=display:flex><span>                    loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>                    history<span style=color:#f92672>.</span>append(loss<span style=color:#f92672>.</span>item())
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>return</span> loss
</span></span><span style=display:flex><span>                optimizer<span style=color:#f92672>.</span>step(closure<span style=color:#f92672>=</span>closure)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> history
</span></span></code></pre></div><p>Defining your geometry in a sensible manner is of huge importance. We will need clear
separation of concerns for our geometry as else we&rsquo;ll be debugging for hours on end: Pytorch and other frameworks
are really good at hiding complexity. Debugging your model later on when you&rsquo;re unsure about the basic definition
can become inconvenient. Thus I highly recommend to visualize your sample and your geometry before optimizing.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>assemble_datasets</span>(self):
</span></span><span style=display:flex><span>        input_sb, output_sb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_spatial_boundary_points() 
</span></span><span style=display:flex><span>        input_tb, output_tb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_temporal_boundary_points()  
</span></span><span style=display:flex><span>        input_int, output_int <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_interior_points()         
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_temporal_boundary_points</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_spatial_boundary_points</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_interior_points</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>show_geometry</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span></code></pre></div><p>Now that we&rsquo;ve got the geometry out of the way, we can compute what our model should converge to
at the boundary. Here, we can either immediately return a residual for our model to
optimize later on. Or we just return our model&rsquo;s output for the boundary as is done
in <code>compute_loss</code> below.
We will maybe have to compute gradients for the boundary. It&rsquo;s nice to have all of this in a
single place, making for easier debugging.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>apply_boundary_conditions</span>(self, input_sb):
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>apply_initial_condition</span>(self, input_tb):
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span></code></pre></div><p>Finally, we can use the ML framework we&rsquo;re in to encode the PDE we want to solve. This is the
&ldquo;interior&rdquo; region where the PDE we defined in the beginning is calculated. Again, taking our
operator L (involving gradients, integrals etc), our source term f. we want its residual to go to zero,
ie.</p><p>$$ L[u]\left( x \right) - f\left(x\right) = r \ \forall x \in O$$</p><p>with (hopefully)</p><p>$$ r \rightarrow 0 $$ the more we train our model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_pde_residual</span>(self, input_int):
</span></span><span style=display:flex><span>        <span style=color:#f92672>....</span>
</span></span></code></pre></div><p>The final loss computation which we need for every training step will look somewhat akin to <code>compute_loss</code>.
Some details: Our model&rsquo;s predictions are returned from the methods we defined above.
Sometimes we also want to leave in some asserts to make sure our tensor manipulations don&rsquo;t quietly destroy
our model.
We get the residual for the interior of our region O as we don&rsquo;t have to compute anything of its variables
anymore.</p><p>Finally, we have an important thing to take care of: Training our model turns out to be a multi-objective
optimization problem. Suddenly the interior PDE and the boundary residuals are in competition to be optimized.
Hence, to have our optimizer fully acknowledge the importance of boundary conditions, we define a weight <code>Lambda</code>
which is chosen arbitrarily such that we solve in the direction of the boundary conditions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(self, inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>        inp_train_sb<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        u_pred_sb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_boundary_conditions(inp_train_sb)
</span></span><span style=display:flex><span>        u_pred_tb <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_initial_condition(inp_train_tb)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> (u_pred_sb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>==</span> u_train_sb<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        r_int <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_pde_residual(inp_train_int)
</span></span><span style=display:flex><span>        r_sb_u_0 <span style=color:#f92672>=</span> u_train_sb[:self<span style=color:#f92672>.</span>n_sb,<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> u_pred_sb[:self<span style=color:#f92672>.</span>n_sb,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        grad_u   <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>grad(u_pred_sb[:,<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sum(), inp_train_sb, create_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        grad_v   <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>grad(u_pred_sb[:,<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>sum(), inp_train_sb, create_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        grad_u_x <span style=color:#f92672>=</span> grad_u[:, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        grad_v_x <span style=color:#f92672>=</span> grad_v[:, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>        r_sb_u_1 <span style=color:#f92672>=</span>  grad_u_x[self<span style=color:#f92672>.</span>n_sb:]
</span></span><span style=display:flex><span>        r_sb_v   <span style=color:#f92672>=</span> grad_v_x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        Lambda <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> Lambda <span style=color:#f92672>*</span> (r_sb_s <span style=color:#f92672>+</span> r_sb_u_0 <span style=color:#f92672>+</span> r_sb_u_1) <span style=color:#f92672>+</span> r_int
</span></span></code></pre></div><h3 id=a-quick-example>A quick example</h3><p>To exemplify how quickly you can adapt this, I tried to build this entire flow using
<a href=https://github.com/tinygrad/tinygrad>tinygrad</a>. Let&rsquo;s quickly define a dense model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Net</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, num_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, dims<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>in_channels  <span style=color:#f92672>=</span> in_channels
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>out_channels <span style=color:#f92672>=</span> out_channels
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>num_layers   <span style=color:#f92672>=</span> num_layers
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>dims         <span style=color:#f92672>=</span> dims
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>input_layer  <span style=color:#f92672>=</span> Linear(in_channels, dims)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>output_layer <span style=color:#f92672>=</span> Linear(dims, out_channels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>activation   <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>tanh
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>layers       <span style=color:#f92672>=</span> [Linear(dims, dims, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>num_layers)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__call__</span>(self, x: Tensor, transform <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> transform <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>forward(x)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>NotImplementedError</span>(<span style=color:#e6db74>&#34;No-transform case not handled yet&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(self<span style=color:#f92672>.</span>input_layer(x))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> l <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>layers:
</span></span><span style=display:flex><span>            x <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>activation(l(x))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>output_layer(x)
</span></span></code></pre></div><p>Now, the PINN model definition flows easily from our existing knowledge of Pytorch.
We use a very simple dense model to approximate the 1+1d heat equation on a rectangular domain. See
the notebooks linked above for comparison.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>PINN</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, n_interior, n_boundary, n_layers<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, n_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2d geometry, heat equation</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>approximate_solution <span style=color:#f92672>=</span> Net(in_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, out_channels<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, num_layers<span style=color:#f92672>=</span>n_layers, dims<span style=color:#f92672>=</span>n_dim)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>domain_extrema       <span style=color:#f92672>=</span> Tensor([[<span style=color:#ae81ff>0.</span>, <span style=color:#ae81ff>0.1</span>], [<span style=color:#f92672>-</span><span style=color:#ae81ff>1.</span>, <span style=color:#ae81ff>1.</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_interior <span style=color:#f92672>=</span> n_interior
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_boundary <span style=color:#f92672>=</span> n_boundary
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>Lambda     <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>spatial_boundary_sample, self<span style=color:#f92672>.</span>temporal_boundary_sample, self<span style=color:#f92672>.</span>interior_sample <span style=color:#f92672>=</span> \
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>assemble_dataset()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@staticmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>convert</span>(x: Tensor, a: float, b: float):
</span></span><span style=display:flex><span>        <span style=color:#75715e># need to adapt to tinygrad&#39;s Uniform sampling mechanism</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># ie. the usual pullback looks a little different</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>.5</span> <span style=color:#f92672>*</span> (b<span style=color:#f92672>-</span>a) <span style=color:#f92672>*</span> (x <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>+</span> a
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>assemble_dataset</span>(self):
</span></span><span style=display:flex><span>        spatial_boundary_input, spatial_boundary_output   <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_spatial_boundary_points()
</span></span><span style=display:flex><span>        temporal_boundary_input, temporal_boundary_output <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_temporal_boundary_points()
</span></span><span style=display:flex><span>        interior_input, interior_output                   <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>add_interior_points()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> (spatial_boundary_input, spatial_boundary_output), (temporal_boundary_input, temporal_boundary_output), (interior_input, interior_output)
</span></span></code></pre></div><p>We can add our geometry and visualize it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_spatial_boundary_points</span>(self):
</span></span><span style=display:flex><span>        x0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        xL <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        t0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        tL <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        points_per_side <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>n_boundary <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        t_values_0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>convert(Tensor<span style=color:#f92672>.</span>uniform(points_per_side,), t0, tL)
</span></span><span style=display:flex><span>        x_values_0 <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>ones(points_per_side) <span style=color:#f92672>*</span> x0
</span></span><span style=display:flex><span>        input_spatial_boundary_0 <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>stack((t_values_0, x_values_0), <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        t_values_L <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>convert(Tensor<span style=color:#f92672>.</span>uniform(points_per_side,), t0, tL)
</span></span><span style=display:flex><span>        x_values_L <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>ones(points_per_side) <span style=color:#f92672>*</span> xL
</span></span><span style=display:flex><span>        input_spatial_boundary_L <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>stack((t_values_L, x_values_L), <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># redundancy for more readable training loop</span>
</span></span><span style=display:flex><span>        output_spatial_boundary_0 <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>zeros(points_per_side)
</span></span><span style=display:flex><span>        output_spatial_boundary_L <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>zeros(points_per_side)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Tensor<span style=color:#f92672>.</span>cat(input_spatial_boundary_0, input_spatial_boundary_L), Tensor<span style=color:#f92672>.</span>cat(output_spatial_boundary_0, output_spatial_boundary_L)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_temporal_boundary_points</span>(self):
</span></span><span style=display:flex><span>        x0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        xL <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        t0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        tL <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        x_values_0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>convert(Tensor<span style=color:#f92672>.</span>uniform(self<span style=color:#f92672>.</span>n_boundary), x0, xL)
</span></span><span style=display:flex><span>        t_values_0 <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>ones(self<span style=color:#f92672>.</span>n_boundary) <span style=color:#f92672>*</span> t0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        input_temporal_boundary  <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>stack((t_values_0, x_values_0), <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        output_temporal_boundary <span style=color:#f92672>=</span> <span style=color:#f92672>-</span> Tensor<span style=color:#f92672>.</span>sin(np<span style=color:#f92672>.</span>pi <span style=color:#f92672>*</span> x_values_0)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> input_temporal_boundary, output_temporal_boundary
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>add_interior_points</span>(self):
</span></span><span style=display:flex><span>        x0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        xL <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        t0 <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        tL <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>domain_extrema[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        t_values_interior <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>convert(Tensor<span style=color:#f92672>.</span>uniform(self<span style=color:#f92672>.</span>n_interior), t0, tL)
</span></span><span style=display:flex><span>        x_values_interior <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>convert(Tensor<span style=color:#f92672>.</span>uniform(self<span style=color:#f92672>.</span>n_interior), x0, xL)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        u_values_interior <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>zeros_like(x_values_interior)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Tensor<span style=color:#f92672>.</span>stack((t_values_interior, x_values_interior), <span style=color:#ae81ff>1</span>), u_values_interior
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>show_sample</span>(self):
</span></span><span style=display:flex><span>        x_in, x_out <span style=color:#f92672>=</span> p<span style=color:#f92672>.</span>add_spatial_boundary_points()
</span></span><span style=display:flex><span>        t_in, t_out <span style=color:#f92672>=</span> p<span style=color:#f92672>.</span>add_temporal_boundary_points()
</span></span><span style=display:flex><span>        interior_in <span style=color:#f92672>=</span> p<span style=color:#f92672>.</span>add_interior_points()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>scatter(x_in[:, <span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy(), x_in[:, <span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy(), )
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>scatter(t_in[:, <span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy(), t_in[:, <span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy(), )
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>scatter(interior_in[:, <span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy(), interior_in[:, <span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy(), )
</span></span><span style=display:flex><span>        plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p>And we can quickly encode the numerics we need to solve the heat equation.
Disclaimer: I&rsquo;m not sure about the correctness of the gradient computation. Will probably
change this when I understand better how tinygrad works.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>apply_initial_condition</span>(self, input_temporal):
</span></span><span style=display:flex><span>        u_pred_temporal <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>approximate_solution(input_temporal)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> u_pred_temporal
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>apply_spatial_condition</span>(self, input_spatial):
</span></span><span style=display:flex><span>        u_pred_spatial <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>approximate_solution(input_spatial)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> u_pred_spatial
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_pde_residual</span>(self, input_interior):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        input_interior<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        x <span style=color:#f92672>=</span> input_interior[:, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        t <span style=color:#f92672>=</span> input_interior[:, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        x<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>; t<span style=color:#f92672>.</span>requires_grad <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        inp <span style=color:#f92672>=</span> Tensor<span style=color:#f92672>.</span>stack((t, x), <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        u <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>approximate_solution(inp)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        u_sum <span style=color:#f92672>=</span> u<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>        u_sum<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        <span style=color:#75715e># get first order derivatives using single backward pass</span>
</span></span><span style=display:flex><span>        grad_u_x <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>        grad_u_t <span style=color:#f92672>=</span> t<span style=color:#f92672>.</span>grad
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># &#34;scalarize&#34; grad_u_x</span>
</span></span><span style=display:flex><span>        intermediate_grad_u_x <span style=color:#f92672>=</span> grad_u_x<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>        intermediate_grad_u_x<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        grad_u_xx <span style=color:#f92672>=</span> intermediate_grad_u_x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        residual <span style=color:#f92672>=</span> grad_u_t <span style=color:#f92672>-</span> grad_u_xx
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> residual<span style=color:#f92672>.</span>realize()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(self, spatial_boundary_input, spatial_boundary_output, temporal_boundary_input,
</span></span><span style=display:flex><span>            temporal_boundary_output, interior_input, interior_output):
</span></span><span style=display:flex><span>        u_pred_spatial  <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_spatial_condition(spatial_boundary_input)
</span></span><span style=display:flex><span>        u_pred_temporal <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>apply_initial_condition(temporal_boundary_input)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        residual_interior <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_pde_residual(interior_input)
</span></span><span style=display:flex><span>        residual_spatial  <span style=color:#f92672>=</span> u_pred_spatial <span style=color:#f92672>-</span> spatial_boundary_output
</span></span><span style=display:flex><span>        residual_temporal <span style=color:#f92672>=</span> u_pred_temporal <span style=color:#f92672>-</span> temporal_boundary_output
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        loss_boundary <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>Lambda <span style=color:#f92672>*</span> ((residual_spatial<span style=color:#f92672>.</span>abs())<span style=color:#f92672>.</span>mean() <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> (residual_temporal<span style=color:#f92672>.</span>abs())<span style=color:#f92672>.</span>mean() <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        loss_interior <span style=color:#f92672>=</span> (residual_interior<span style=color:#f92672>.</span>abs())<span style=color:#f92672>.</span>mean() <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> loss_boundary <span style=color:#f92672>+</span> loss_interior
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> loss
</span></span></code></pre></div><p>And we&rsquo;re done. This can now be tuned to fully converge to the analytical solution.</p><h3 id=final-thoughts>Final thoughts</h3><p>There&rsquo;s lots to discover in this new world of SciML: Solving non-deterministic models in high dimensions,
operator learning, foundation models, PDE-constrained optimization &mldr; We&rsquo;re seeing the fruits of an intense
decade of investment in ML research, enabling us to solve hard problems on retail machines in no time.</p><p>Thanks to Professor Mishra and colleagues, you can follow the lecture series on <a href="https://www.youtube.com/watch?v=y6wHpRzhhkA&amp;ab_channel=CAMLab%2CETHZ%C3%BCrich">Deep Learning in Scientific
Computing</a> to
get <em>all</em> the details.</p><p><em>Note (July 14th 2023)</em>: Click <a href=https://github.com/konradha/dlsc_project23>here</a> for a little more advanced
implementation.</p></section><nav class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"><a class="ltr:pr-3 rtl:pl-3" href=https://konradha.com/posts/affinity/><span class="ltr:mr-1.5 rtl:ml-1.5">←</span><span>Affinity</span></a>
<a class="ltr:ml-auto rtl:mr-auto justify-end pl-3" href=https://konradha.com/posts/surface_code_note/><span>Minimum weight error correction in the surface code</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a></nav></article></main><footer class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"><div class=mr-auto>&copy; 2025
<a class=link href=https://konradha.com/>א</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>powered by hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>hugo-paper</a></footer></body></html>