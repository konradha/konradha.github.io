<!doctype html><html class="not-ready lg:text-base" style=--bg:#f8f5d7 lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>On the necessity of removing the 𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕 requirement in traceable functional collectives - א</title><meta name=theme-color><meta name=description content="Recent Pytorch-related releases
might make some of the discussion here
irrelevant. I do think that people &ndash; as long as the experimental features recently
published aren&rsquo;t in Pytorch main &ndash; will keep relying on Pytorch&rsquo;s compilation
capabilities (e.g. vLLM).
Overall, this post is me trying to get some ideas across and present my current understanding.
Happy to receive feedback on where my understanding is off or what can be investigated further!"><meta name=author content="א"><link rel="preload stylesheet" as=style href=https://konradha.com/main.min.css><link rel=preload as=image href=https://konradha.com/theme.png><script defer src=https://konradha.com/highlight.min.js onload=hljs.initHighlightingOnLoad()></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://konradha.com/favicon.ico><link rel=apple-touch-icon href=https://konradha.com/apple-touch-icon.png><meta name=generator content="Hugo 0.152.2"><meta itemprop=name content="On the necessity of removing the 𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕 requirement in traceable functional collectives"><meta itemprop=description content="Recent Pytorch-related releases might make some of the discussion here irrelevant. I do think that people – as long as the experimental features recently published aren’t in Pytorch main – will keep relying on Pytorch’s compilation capabilities (e.g. vLLM).
Overall, this post is me trying to get some ideas across and present my current understanding. Happy to receive feedback on where my understanding is off or what can be investigated further!"><meta itemprop=datePublished content="2025-10-26T19:45:36+01:00"><meta itemprop=dateModified content="2025-10-26T19:45:36+01:00"><meta itemprop=wordCount content="943"><meta property="og:url" content="https://konradha.com/posts/necessary-compilation/"><meta property="og:site_name" content="א"><meta property="og:title" content="On the necessity of removing the 𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕 requirement in traceable functional collectives"><meta property="og:description" content="Recent Pytorch-related releases might make some of the discussion here irrelevant. I do think that people – as long as the experimental features recently published aren’t in Pytorch main – will keep relying on Pytorch’s compilation capabilities (e.g. vLLM).
Overall, this post is me trying to get some ideas across and present my current understanding. Happy to receive feedback on where my understanding is off or what can be investigated further!"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-26T19:45:36+01:00"><meta property="article:modified_time" content="2025-10-26T19:45:36+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="On the necessity of removing the 𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕 requirement in traceable functional collectives"><meta name=twitter:description content="Recent Pytorch-related releases might make some of the discussion here irrelevant. I do think that people – as long as the experimental features recently published aren’t in Pytorch main – will keep relying on Pytorch’s compilation capabilities (e.g. vLLM).
Overall, this post is me trying to get some ideas across and present my current understanding. Happy to receive feedback on where my understanding is off or what can be investigated further!"><link rel=canonical href=https://konradha.com/posts/necessary-compilation/><script data-goatcounter=https://konradha.goatcounter.com/count async src=//gc.zgo.at/count.js></script></head><link rel=stylesheet href=/css/fix.css><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center"><div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center"><a class="-translate-y-[1px] text-2xl font-medium" href=https://konradha.com/>א</a><div class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#f8f5d7".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"><a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"><article><header class=mb-14><h1 class="!my-0 pb-2.5">On the necessity of removing the 𝚏𝚞𝚗𝚌𝚝𝚒𝚘𝚗𝚊𝚕 requirement in traceable functional collectives</h1><div class="text-xs antialiased opacity-60"><time>Oct 26, 2025</time></div></header><section><p>Recent Pytorch-related <a href=https://pytorch.org/blog/torchcomms/>releases</a>
might make some of the discussion here
irrelevant. I do think that people &ndash; as long as the experimental features recently
published aren&rsquo;t in Pytorch main &ndash; will keep relying on Pytorch&rsquo;s compilation
capabilities (e.g. vLLM).</p><p>Overall, this post is me trying to get some ideas across and present my current understanding.
Happy to receive feedback on where my understanding is off or what can be investigated further!</p><p>There is ample amount of content to deep-dive into the Pytorch codebase. We will not assume
proficiency but overall familiarity with Pytorch and compilation here.</p><h4 id=introduction>Introduction</h4><p>I&rsquo;ve recently opened a <a href=https://github.com/pytorch/pytorch/pull/161213>PR</a>
to allow for Pytorch&rsquo;s compilation pipeline to understand pointwise communications.</p><p>Curious, one might say, as most operators we pass through (either forwards or
during autodiff) are collectives, so why would you want this specific feature?
Different applications
come to mind. I will offer two applications that are often used in HPC and ML.</p><ol><li><p><a href=https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture25.pdf>Halo exchanges</a></p></li><li><p><a href=https://arxiv.org/abs/2310.01889>RingAttention</a> & friends.</p></li></ol><h4 id=current-state>Current state</h4><p>One could argue that using NCCL&rsquo;s <a href=https://dev-discuss.pytorch.org/t/pytorch-symmetricmemory-harnessing-nvlink-programmability-with-ease/2798>SymmetricMemory</a>
feature might be sufficient
We want to squeeze any and all overhead and want to enable
optimizations allowing for the scheduler infrastructure and <a href=https://dev-discuss.pytorch.org/t/understanding-cudagraph-trees/1967/2>cudagraphs</a>
to capture all paths that lead to lower walltime. Exploiting Pytorch&rsquo;s compilation
pipeline fully is for me then the proper channel to do that.</p><p>There&rsquo;s been a considerable push for functionalization in Pytorch&rsquo;s infrastructure to reduce
complexity when reasoning about the different compilation stages, e.g. <a href=https://github.com/vllm-project/vllm/issues/14703>here</a> or <a href=https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965>here</a>. To that end, the collectives have been designed
to be fully functional, see the <a href=https://docs.google.com/document/d/1Jqa68gvuVeFWZJFOiukmb58jAaUEET1GVMkd1GOMRT4>Traceable functional collectives design document</a>.</p><p>However, here&rsquo;s an immediate issue. Consider the following function.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>ops<span style=color:#f92672>.</span>_c10d_functional<span style=color:#f92672>.</span>all_reduce_coalesced_([x], <span style=color:#e6db74>&#34;sum&#34;</span>,)
</span></span></code></pre></div><p>As is per the usual Pytorchisms, the suffixed underscore indicates an in-place operation.
Even though this function call technically makes part of the traceable functional collectives
infrastructure, this will yield an error upon calling (any option) of <code>torch.compile</code> on it.
(Technically you can call <code>torch.compile</code> on it but any subsequent execution of the compiled
function will yield an error).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>RuntimeError: Found a custom <span style=color:#f92672>(</span>non-ATen<span style=color:#f92672>)</span> operator whose output has alias annotations: _c10d_functional::all_reduce_coalesced_<span style=color:#f92672>(</span>Tensor<span style=color:#f92672>[](</span>a!<span style=color:#f92672>)</span> inputs, str reduce_op, str group_name<span style=color:#f92672>)</span> -&gt; Tensor<span style=color:#f92672>[](</span>a!<span style=color:#f92672>)</span>. We only support functionalizing operators whose outputs <span style=color:#66d9ef>do</span> not have alias annotations
</span></span></code></pre></div><p>In CS speak, in-place ops induce side effects. Side effects notably are forbidden in functional
paradigms. I&rsquo;m not entirely sure inductor&rsquo;s scheduler tracks down any collective call
to reduce the number of copies &ndash; there is a small number of <a href=https://github.com/pytorch/pytorch/issues/134388>issues</a>
suggesting otherwise.</p><h4 id=my-changes>My changes</h4><p>In my <a href=https://github.com/pytorch/pytorch/pull/161213>PR</a>, I&rsquo;ve sort of abused the
existing infrastructure for collectives to allow for pointwise communications to be
traced using Dynamo. Notably, we construct temporary nodes in the graph for <code>P2POps</code>
which are often used together with <code>batch_isend_irecv</code> to issue coalesced pointwise
comms. The entire idea of this batching is to allow reducing overhead when creating new
communicators (highly recommend looking inside <code>torch/csrc/distributed/c10d</code> and
<a href=https://github.com/pytorch/pytorch/blob/main/torch/csrc/cuda/nccl.cpp>e.g. this file</a> for
deeper information).</p><p>We build <code>P2POpVariable</code>s for <code>P2POp</code>s that are verbosely passing the information to the
compiled graph such that this</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>work <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>batch_isend_irecv(
</span></span><span style=display:flex><span>        [
</span></span><span style=display:flex><span>          dist<span style=color:#f92672>.</span>P2POp(dist<span style=color:#f92672>.</span>isend, x, <span style=color:#ae81ff>0</span>),
</span></span><span style=display:flex><span>          dist<span style=color:#f92672>.</span>P2POp(dist<span style=color:#f92672>.</span>recv, y, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>some_work(x, z)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> work: w<span style=color:#f92672>.</span>wait()
</span></span></code></pre></div><p>can become something more akin to</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tensors_to_wait <span style=color:#f92672>=</span> ops.batch_p2p_ops<span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>[</span><span style=color:#e6db74>&#34;isend&#34;</span>, <span style=color:#e6db74>&#34;irecv&#34;</span><span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>[</span>0, 1<span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>[</span>x, y<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>some_work_compiled<span style=color:#f92672>(</span>x, z<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> t in tensors_to_wait:
</span></span><span style=display:flex><span>    dist.wait_tensor<span style=color:#f92672>(</span>t<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>In my PR, none of the ops appear in-place and we induce extra copies
to allow for the operations to take place, see in
<code>torch.distributed._functional_collectives.py</code>
(compare the notation to what the error before told us about in-place operations):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;batch_p2p_ops(str[] op_list, int[] peer_list, int[] tag_list, Tensor[] tensors, str group_name) -&gt; Tensor[]&#34;</span>
</span></span></code></pre></div><p>As a WIP this is fine &ndash; to allow for efficient interleaving and reducing expensive
data movement we would want zero-copy comms though! I still want to change it to be
fully in-place &ndash; as we&rsquo;ve seen earlier, this is one of the reasons why we want to
relax the <code>functional</code> requirement in the traceable functional collectives infrastructure.</p><h4 id=changes-inducing-bugs>Changes inducing bugs</h4><p>So far it&rsquo;s great, we can use this async comms feature now to do distributed convolutions
and whatever involves <em>blocking</em> halo exchanges. What if we want to interleave
communication and computation?</p><p>Consider this code snippet:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>kernel</span>(x0, x1, y0, y1):
</span></span><span style=display:flex><span>  r <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>get_rank()
</span></span><span style=display:flex><span>  w <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>get_world_size()
</span></span><span style=display:flex><span>  nxt <span style=color:#f92672>=</span> (r <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> w
</span></span><span style=display:flex><span>  prv <span style=color:#f92672>=</span> (r <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>%</span> w
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  work <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>batch_isend_irecv([
</span></span><span style=display:flex><span>      dist<span style=color:#f92672>.</span>P2POp(dist<span style=color:#f92672>.</span>isend, x0, nxt),
</span></span><span style=display:flex><span>      dist<span style=color:#f92672>.</span>P2POp(dist<span style=color:#f92672>.</span>irecv, y0, prv),
</span></span><span style=display:flex><span>  ])
</span></span><span style=display:flex><span>  t0 <span style=color:#f92672>=</span> x0 <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> ww <span style=color:#f92672>in</span> work: ww<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>  a <span style=color:#f92672>=</span> y0 <span style=color:#f92672>+</span> t0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  work <span style=color:#f92672>=</span> dist<span style=color:#f92672>.</span>batch_isend_irecv([
</span></span><span style=display:flex><span>      dist<span style=color:#f92672>.</span>P2POp(dist<span style=color:#f92672>.</span>isend, a, nxt),
</span></span><span style=display:flex><span>      dist<span style=color:#f92672>.</span>P2POp(dist<span style=color:#f92672>.</span>irecv, y1, prv),
</span></span><span style=display:flex><span>  ])
</span></span><span style=display:flex><span>  t1 <span style=color:#f92672>=</span> a <span style=color:#f92672>*</span> <span style=color:#ae81ff>1.000244140625</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> ww <span style=color:#f92672>in</span> work: ww<span style=color:#f92672>.</span>wait()
</span></span><span style=display:flex><span>  out <span style=color:#f92672>=</span> y1 <span style=color:#f92672>+</span> t1
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> out
</span></span></code></pre></div><p>It&rsquo;s unfortunately a little involved but it&rsquo;ll show us: The current implementation
fails to correctly take care of dependencies wrt. data flow.
Comparing eager and compiled runs of this function will yield garbage differences
indicating something is off.</p><p>Let&rsquo;s investigate:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>TORCH_COMPILE_DEBUG<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  TORCH_LOGS<span style=color:#f92672>=</span>output_code <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  TORCHINDUCTOR_CACHE_DIR<span style=color:#f92672>=</span>$PWD/inductor_cache/$RANK <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  TRITON_CACHE_DIR<span style=color:#f92672>=</span>$PWD/triton_cache/$RANK <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  torchrun --nproc-per-node<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span> --standalone tester.py
</span></span><span style=display:flex><span><span style=color:#75715e># tester contains the above snippet and some numerics checks</span>
</span></span></code></pre></div><p>Inspecting the inductor cache (here, explicitly set in the flags,
<code>inductor_cache/4y/c4yjdzyq3kj4da7e2xi5gnbvzd4laxipv66ohspyo4nuecce37pv.py</code>):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>buf0 <span style=color:#f92672>=</span> _c10d_functional<span style=color:#f92672>.</span>batch_p2p_ops([<span style=color:#e6db74>&#39;isend&#39;</span>,<span style=color:#e6db74>&#39;irecv&#39;</span>], [<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>], [arg0_1, arg1_1], <span style=color:#e6db74>&#39;0&#39;</span>)
</span></span><span style=display:flex><span>buf1 <span style=color:#f92672>=</span> buf0[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>buf2 <span style=color:#f92672>=</span> buf0[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>wait_tensor(buf1); wait_tensor(buf2)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>triton_poi_fused_add_mul_0<span style=color:#f92672>.</span>run(arg1_1, arg0_1, arg2_1, buf7, buf15, <span style=color:#ae81ff>1048576</span>, stream<span style=color:#f92672>=</span>stream1)
</span></span></code></pre></div><p>and only after the kernel call, it reads</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>buf8 <span style=color:#f92672>=</span> _c10d_functional<span style=color:#f92672>.</span>batch_p2p_ops([<span style=color:#e6db74>&#39;isend&#39;</span>,<span style=color:#e6db74>&#39;irecv&#39;</span>], [<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>0</span>], [<span style=color:#ae81ff>0</span>,<span style=color:#ae81ff>0</span>], [buf7, arg2_1], <span style=color:#e6db74>&#39;0&#39;</span>)
</span></span><span style=display:flex><span>buf9 <span style=color:#f92672>=</span> buf8[<span style=color:#ae81ff>0</span>]; buf10 <span style=color:#f92672>=</span> buf8[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>wait_tensor(buf9); wait_tensor(buf10)
</span></span></code></pre></div><p>Verdict: Inductor hoisted calls to a wrong argument. Inductor does not carry dependency correctly.
Surely, we can try and track down the wrongly-assigned dependencies here.
Or: Would making TFC allow for side effects be a smart choice to allow for Inductor to do it?</p><p>We would get zero-copy <code>all_reduce</code> and my newly-introduced pointwise
comms would compile much more easily.</p></section><nav class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"><a class="ltr:ml-auto rtl:mr-auto justify-end pl-3" href=https://konradha.com/posts/gpu-animation/><span>An Animation</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a></nav></article></main><footer class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"><div class=mr-auto>&copy; 2025
<a class=link href=https://konradha.com/>א</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>powered by hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>hugo-paper</a></footer></body></html>