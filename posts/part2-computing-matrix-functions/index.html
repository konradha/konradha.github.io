<!doctype html><html class="not-ready lg:text-base" style=--bg:#f8f5d7 lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Computing Matrix Functions - א</title><meta name=theme-color><meta name=description content="This is the second part of a series of posts exploring
how to make exponential integrators fast.
In our
previous exploration
we came to understand what &ldquo;nice&rdquo; integrators for integrable systems can
look like. How does one actually compute these trigonometric matrix functions in practice?
Let&rsquo;s reformulate our current issue as: We want to compute an update for our numerical
integration in the temporal dimension, ie. (short form here)
$$u(t + \tau) \approx \exp(\tau A) u(t)$$"><meta name=author content="א"><link rel="preload stylesheet" as=style href=https://konradha.com/main.min.css><link rel=preload as=image href=https://konradha.com/theme.png><script defer src=https://konradha.com/highlight.min.js onload=hljs.initHighlightingOnLoad()></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://konradha.com/favicon.ico><link rel=apple-touch-icon href=https://konradha.com/apple-touch-icon.png><meta name=generator content="Hugo 0.147.6"><meta itemprop=name content="Computing Matrix Functions"><meta itemprop=description content="This is the second part of a series of posts exploring how to make exponential integrators fast.
In our previous exploration we came to understand what “nice” integrators for integrable systems can look like. How does one actually compute these trigonometric matrix functions in practice?
Let’s reformulate our current issue as: We want to compute an update for our numerical integration in the temporal dimension, ie. (short form here)
$$u(t + \tau) \approx \exp(\tau A) u(t)$$"><meta itemprop=datePublished content="2025-02-04T18:00:15+01:00"><meta itemprop=dateModified content="2025-02-04T18:00:15+01:00"><meta itemprop=wordCount content="1487"><meta property="og:url" content="https://konradha.com/posts/part2-computing-matrix-functions/"><meta property="og:site_name" content="א"><meta property="og:title" content="Computing Matrix Functions"><meta property="og:description" content="This is the second part of a series of posts exploring how to make exponential integrators fast.
In our previous exploration we came to understand what “nice” integrators for integrable systems can look like. How does one actually compute these trigonometric matrix functions in practice?
Let’s reformulate our current issue as: We want to compute an update for our numerical integration in the temporal dimension, ie. (short form here)
$$u(t + \tau) \approx \exp(\tau A) u(t)$$"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-04T18:00:15+01:00"><meta property="article:modified_time" content="2025-02-04T18:00:15+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Computing Matrix Functions"><meta name=twitter:description content="This is the second part of a series of posts exploring how to make exponential integrators fast.
In our previous exploration we came to understand what “nice” integrators for integrable systems can look like. How does one actually compute these trigonometric matrix functions in practice?
Let’s reformulate our current issue as: We want to compute an update for our numerical integration in the temporal dimension, ie. (short form here)
$$u(t + \tau) \approx \exp(\tau A) u(t)$$"><link rel=canonical href=https://konradha.com/posts/part2-computing-matrix-functions/></head><link rel=stylesheet href=/css/fix.css><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center"><div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center"><a class="-translate-y-[1px] text-2xl font-medium" href=https://konradha.com/>א</a><div class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#f8f5d7".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"><a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"><article><header class=mb-14><h1 class="!my-0 pb-2.5">Computing Matrix Functions</h1><div class="text-xs antialiased opacity-60"><time>Feb 4, 2025</time></div></header><section><p><em>This is the second part of a series of posts exploring
how to make exponential integrators fast.</em></p><p>In our
<a href=https://konradha.com/posts/part1-exponential-integrators/>previous exploration</a>
we came to understand what &ldquo;nice&rdquo; integrators for integrable systems can
look like. How does one actually compute these trigonometric matrix functions in practice?</p><p>Let&rsquo;s reformulate our current issue as: We want to compute an update for our numerical
integration in the temporal dimension, ie. (short form here)</p><p>$$u(t + \tau) \approx \exp(\tau A) u(t)$$</p><p>Let&rsquo;s look at what <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.expm.html>Scipy does</a>.
Relying on Higham&rsquo;s and Al-Mohy&rsquo;s work it states</p><blockquote><p>essentially a Pade approximation with a variable order that is decided based on the array data</p></blockquote><p>Ok, cool. Can we do that as well, translating, optimizing this for the matrices arising in our problem?
Let&rsquo;s inspect our matrix more precisely.</p><p>Using a finite differences, first-order approximation of our discrete Laplacian $\bar{\Delta}$
with no-flux boundary conditions is going to be tridiagonal, symmetric with 2 more bands on the
$+/- n_x$ diagonals. So we have $n + 2 (n - 1) + 2 (n - nx)$ nonzeros total. Would we then
save the entire dense matrix to store all elements? We could make use of the plethora of sparse
matrix formats in existence: CSR, COO,
<a href=https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.sparse.BCSR.html>BCSR</a> &mldr;
<a href=https://docs.nvidia.com/nvpl/latest/sparse/storage_format/sparse_matrix.html>take your pick</a>.</p><p>Computing the k-th power of such a matrix won&rsquo;t preserve the sparsity pattern however. So,
expanding $\exp\left(\tau A\right) = \sum_{k=0}^{\infty} \frac{\left(\tau A\right)^{k}}{k!} $
is, in general, dense. As an aside, $\bar{\Delta}^{-1}$ will, in general, not be dense either.
One can observe this fact by looking at the spectrum of $\bar{\Delta}$ and how the its inverse
would decay. We can derive a closed-form solution to $A - \lambda I = 0$ where $A = \bar{\Delta}$
and reason from this result.</p><p>So, well, if we don&rsquo;t want to actually have a huge not-so-sparse object in memory to compute for
every time step &ndash; is it actually feasible?</p><p>We&rsquo;ll need some tools from linear algebra and complex analysis to actually build this.</p><h4 id=krylov-subspaces>Krylov subspaces</h4><p>A Krylov subspace $\mathcal{K}_l$ is defined from a matrix-vector pair $(A, u)$, $A \in \mathbb{K}^{m, n}$ and
$u \in \mathbb{K}^{m}$:</p><p>$$\mathcal{K}_l := \text{span}[u, Au, &mldr; A^{l-1} u ]$$</p><p>It&rsquo;s a richly developed theory in numerical linear algebra to give us some background on the question:
If A is sparse, difficult to handle &ndash; how can we nicely approximate
$f(tA) u$ without making too many &ldquo;bad decisions&rdquo; in terms of performance and correctness?</p><p>Applying the sparse matrix-vector products here yields something that looks similar to
what the above span offers. However: To be sure that we can actually use this, we need to come up with
a procedure that can generate a basis for this Krylov space. Ie. for any element in our newly-built basis $\mathcal{B}$
we require something like $\langle v_i, v_j \rangle = 0$ for $i \neq j, v_i \in \mathcal{B}$.</p><p>That&rsquo;s what the iterations related to Arnoldi and Lanczos processes are about.</p><h4 id=arnoldi-iterations>Arnoldi iterations</h4><p>Originally, this type of process had been constructed to approximate an inverse computation that arose often
in the earlier days of scientific computing. Well, it still arises, historically it&rsquo;s just from that area.</p><p>$$\left(\lambda I - A\right)^{-1} v$$</p><p>ie. approximating the inverse of a matrix was the main interest for this method.
The iteration finds matrices $V_m, H_m$. It might also have to find a normalization factor $\beta$ to correctly
project into the actual space we can find $A$ in, more on that later.</p><p>$V_m$ is an orthonormal basis of the Krylov space required.
$H_m$ is a Hessenberg matrix. Both are of small size and we can finally get to understand this ominous parameter $m$.
It denotes the number of iterations we need to run this process for to get a &ldquo;good&rdquo; result &ndash; and also the matrix shapes
of $V, H$.</p><p>Now, the iteration approximates the inverse computation via</p><p>$$\left(\lambda I - A\right)^{-1} \approx V_m (\lambda I - H_m)^{-1} e_1$$ where $e_i$ denotes the i-th basis vector
of $\mathcal{R}^{n}$.
It&rsquo;s a general eigenvalue problem being posed.
The form suggested is</p><p>$$A V_m = V_m H_m + h_{m+1, m} v_{m+1} e_m^{T}$$</p><p>The above approximation holds if the spectrum of $A$ is different than the one from $H_m$
and in general if $\lambda \notin \sigma(A), \lambda \notin \sigma(H_m)$.</p><p>Let&rsquo;s define a set</p><p>$$\mathcal{F} (A) := \left[x^* A x: x \in \mathbf{C}^n, ||x|| = 1 \right]$$</p><p>which is crucial to show that the above considerations of $\lambda$&rsquo;s spectrum hold.</p><p>Complex analysis gives us access to compute the matrix function via contour integral: Let $f$ be a function that&rsquo;s
analytic around the neighborhood of $\mathcal{F} (A)$:</p><p>$$f(A) v = \frac{1}{2 \pi i} \int_\Gamma f(\lambda) (\lambda I - A)^{-1}v d\lambda$$</p><p>where $\Gamma$ is a contour surrounding $\mathcal{F} (A)$.</p><p>But wait, now we can identify terms from above and exchange them!</p><p>$$\frac{1}{2 \pi i} \int_\Gamma f(\lambda) V_m (\lambda I - H_m)^{-1} e_1 d\lambda = V_m f(H_m) e_1$$</p><p>Finally, we have an expression for our approximation!</p><p>$$f(A)v \approx V_m f(H_m) e_1$$</p><p>If we assume $A$ to have properties such as positive definiteness and symmetry we can even realize the following:
$H_m$ is a symmetric Hessenberg matrix and thus necessarily tridiagonal. Which is easily diagonalizable:
Hence we can write, without further ado:</p><p>$$f(A)v \approx V_m Q^* f(S_m) Q e_1$$</p><p>where $H_m = Q^* S_m Q$ and $S_m$ is a diagonal matrix containing the $m$ eigenvalues of $H_m$, $Q$ containing
$H_m$&rsquo;s eigenvectors.</p><p>Here we&rsquo;ve jumped quite a few steps &ndash; I&rsquo;ll refer the interested reader to the elaborations mentioned in the end of this
article. Assumptions on symmetry are of course not very general. We&rsquo;re however allowed to do that in our
highly-symmetric case, as our discrete operator $\bar{\Delta}$ is so regular.
This lets us exploit a subclass of Arnoldi processes, namely so-called Lanczos iterations where we can exploit this
symmetry when implementing the iteration. For numerical stability reasons we can always introduce some orthogonalization
procedure. Let&rsquo;s see what this finally looks like in practice.</p><h4 id=implementation>Implementation</h4><p>We&rsquo;ll be making use of Eigen here. Observe the specialized function calls
for this complex-type procedure (ie the calls to <code>adjoint()</code>).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>template</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>typename</span> Float<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>std<span style=color:#f92672>::</span>tuple<span style=color:#f92672>&lt;</span>Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span>, Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span>, Float<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>lanczos_L(<span style=color:#66d9ef>const</span> Eigen<span style=color:#f92672>::</span>SparseMatrix<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> <span style=color:#f92672>&amp;</span>L, <span style=color:#66d9ef>const</span> Eigen<span style=color:#f92672>::</span>VectorX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> <span style=color:#f92672>&amp;</span>u,
</span></span><span style=display:flex><span>          <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>uint32_t</span> m) {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>uint32_t</span> n <span style=color:#f92672>=</span> L.rows();
</span></span><span style=display:flex><span>  Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> V <span style=color:#f92672>=</span> Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;::</span>Zero(n, m);
</span></span><span style=display:flex><span>  Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> T <span style=color:#f92672>=</span> Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;::</span>Zero(m, m);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  Float beta <span style=color:#f92672>=</span> u.norm();
</span></span><span style=display:flex><span>  V.col(<span style=color:#ae81ff>0</span>) <span style=color:#f92672>=</span> u <span style=color:#f92672>/</span> beta;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>uint32_t</span> j <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; j <span style=color:#f92672>&lt;</span> m <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>; j<span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>    Eigen<span style=color:#f92672>::</span>VectorX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> w <span style=color:#f92672>=</span> L <span style=color:#f92672>*</span> V.col(j);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (j <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>      w <span style=color:#f92672>-=</span> T(j <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>, j) <span style=color:#f92672>*</span> V.col(j <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>    T(j, j) <span style=color:#f92672>=</span> V.col(j).adjoint() <span style=color:#f92672>*</span> w;
</span></span><span style=display:flex><span>    w <span style=color:#f92672>-=</span> T(j, j) <span style=color:#f92672>*</span> V.col(j);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Modified Gram-Schmidt orthogonalization
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>uint32_t</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;=</span> j; i<span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>      Float coeff <span style=color:#f92672>=</span> V.col(i).adjoint() <span style=color:#f92672>*</span> w;
</span></span><span style=display:flex><span>      w.noalias() <span style=color:#f92672>-=</span> coeff <span style=color:#f92672>*</span> V.col(i);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    T(j <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, j) <span style=color:#f92672>=</span> w.norm();
</span></span><span style=display:flex><span>    T(j, j <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>=</span> T(j <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, j);
</span></span><span style=display:flex><span>    <span style=color:#75715e>// could use early stopping \approx beta
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>    V.col(j <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>=</span> w <span style=color:#f92672>/</span> T(j <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, j);
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> {V, T, beta};
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This now gives us access to $H_m, V_m$. Nice. Finally, we can approximate our matrix function
for a given vector like so</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#66d9ef>template</span> <span style=color:#f92672>&lt;</span><span style=color:#66d9ef>typename</span> Float<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>Eigen<span style=color:#f92672>::</span>VectorX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> expm_multiply(<span style=color:#66d9ef>const</span> Eigen<span style=color:#f92672>::</span>SparseMatrix<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> <span style=color:#f92672>&amp;</span>L,
</span></span><span style=display:flex><span>                                    <span style=color:#66d9ef>const</span> Eigen<span style=color:#f92672>::</span>VectorX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> <span style=color:#f92672>&amp;</span>u, Float t,
</span></span><span style=display:flex><span>                                    <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>uint32_t</span> m <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>) {
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>const</span> <span style=color:#66d9ef>auto</span> [V, T, beta] <span style=color:#f92672>=</span> lanczos_L(L, u, m);
</span></span><span style=display:flex><span>  Eigen<span style=color:#f92672>::</span>SelfAdjointEigenSolver<span style=color:#f92672>&lt;</span>Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;&gt;</span> es(T);
</span></span><span style=display:flex><span>  Eigen<span style=color:#f92672>::</span>MatrixX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> exp_T <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>      (es.eigenvectors() <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>       (t <span style=color:#f92672>*</span> es.eigenvalues().array().abs())
</span></span><span style=display:flex><span>           .unaryExpr([](Float x) { <span style=color:#66d9ef>return</span> std<span style=color:#f92672>::</span>exp(x); })
</span></span><span style=display:flex><span>           .matrix()
</span></span><span style=display:flex><span>           .asDiagonal() <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>       es.eigenvectors().transpose());
</span></span><span style=display:flex><span>  Eigen<span style=color:#f92672>::</span>VectorX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;</span> e1 <span style=color:#f92672>=</span> Eigen<span style=color:#f92672>::</span>VectorX<span style=color:#f92672>&lt;</span>Float<span style=color:#f92672>&gt;::</span>Zero(T.rows());
</span></span><span style=display:flex><span>  e1(<span style=color:#ae81ff>0</span>) <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span>;
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> beta <span style=color:#f92672>*</span> V <span style=color:#f92672>*</span> exp_T <span style=color:#f92672>*</span> e1;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>An unfortunately very dense snippet. The main action happens inside the chained action on the eigensolver:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>    (es.eigenvectors() <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>       (t <span style=color:#f92672>*</span> es.eigenvalues().array().abs())
</span></span><span style=display:flex><span>           .unaryExpr([](Float x) { <span style=color:#66d9ef>return</span> std<span style=color:#f92672>::</span>exp(x); })
</span></span><span style=display:flex><span>           .matrix()
</span></span><span style=display:flex><span>           .asDiagonal() <span style=color:#f92672>*</span>
</span></span><span style=display:flex><span>       es.eigenvectors().transpose());
</span></span></code></pre></div><p>where we apply
$$Q \left(t \exp(S_m) \right) Q^{T} $$</p><p>and then scale it back using our normalization factor $\beta$ computed during the Lanczos iteration before.
The exponential function application happens element-wise for the diagonal matrix $S$.</p><p>It&rsquo;s important to note that this function definition is not perf-ready: we allocate lots of memory for every
function call. Ideally, when calling this repeatedly (ie. to wiggle $u(t)$ to $u(t + \tau)$), we would
want the function to not present any obvious bottlenecks.</p><h4 id=recap>Recap</h4><p>We saw that it&rsquo;s problematic to store functions of large matrices in memory. Especially if the function is a
sum of terms that &ldquo;densify&rdquo; our structure.</p><p>We&rsquo;ve seen a procedure that can project the &ldquo;interesting parts&rdquo; of a sparse linear operator into a smaller subspace.
We then observed that this can be projected back into the space the original space, the function application
happening in between. This makes it feasible for us to find an approximation of the matrix function to finally do what
we wanted, ie. twist and turn our solution vector from one time step to another. &ldquo;Twisting and turning&rdquo; here meaning
turning over $n$ orthogonal directions.</p><p>Next time we&rsquo;ll plug this into some larger structure to show that we can employ this procedure for actual simulations.
And create a few nice first films from it.</p></section><nav class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"><a class="ltr:ml-auto rtl:mr-auto justify-end pl-3" href=https://konradha.com/posts/part1-exponential-integrators/><span>Exponential Integrators for PDEs</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a></nav></article></main><footer class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"><div class=mr-auto>&copy; 2025
<a class=link href=https://konradha.com/>א</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>powered by hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>hugo-paper</a></footer></body></html>